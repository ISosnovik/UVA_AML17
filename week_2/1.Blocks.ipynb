{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "## Blocks\n",
    "\n",
    "The main idea of this assignment is to allow you to undestand how **neural networks** (NNs) work. We will cover the main aspects such as the **Backpropagation** and **Optimization Methods**. All mathematical operations should be implemented in **NumPy** only. \n",
    "\n",
    "The assignmnent consist of 2 notebooks:\n",
    "* *Blocks* - the place where the main **building blocks** of the NNs should be implemented\n",
    "* *Experiments* - a playground. There we will train the models\n",
    "\n",
    "### Note\n",
    "Some of the concepts below have not (yet) been discussed durin the lecture. These will be discussed further during the next Wednesday lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* [0. Preliminaries](#0.-Preliminaries)\n",
    "* [1. Backpropagation](#1.-Backpropagation)\n",
    "* [2. Dense layer](#2.-Dense-layer)\n",
    "* [3. ReLU nonlinearity](#3.-ReLU-nonlinearity)\n",
    "* [4. Sequential model](#4.-Sequential-model)\n",
    "* [5. Loss](#5.-Loss)\n",
    "* [6. $L_2$ Regularization & Weight Decay](#6.-$L_2$-Regularization-&-Weight-Decay)\n",
    "* [7. Optimizer](#7.-Optimizer)\n",
    "* [8. Advanced blocks](#8.-Advanced-blocks)\n",
    "    * [8.1 Dropout](#8.1-Dropout)\n",
    "    * [8.2 MSE Loss](#8.2-MSE-Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preliminaries\n",
    "In this assignment we will use **classes** and their **instances** (objects). It will allow us to write less code and make it more readable. However, you don't have to take care about the exact implementation of the classes. We did it for you. \n",
    "But if you are interested in it, here are some useful links:\n",
    "* The official [documentation](https://docs.python.org/3/tutorial/classes.html) \n",
    "* Video by *sentdex*: [Object Oriented Programming Introduction](https://www.youtube.com/watch?v=ekA6hvk-8H8)\n",
    "* Antipatterns in OOP: [Stop Writing Classes](https://www.youtube.com/watch?v=o9pEzgHorH0)\n",
    "\n",
    "The interface of the current blocks is mostly inspired by **[Torch](http://torch.ch) / [PyTorch](http://pytorch.org)**. You can also take a look at the first implementation of [Keras](https://github.com/fchollet/keras/tree/37a1db225420851cc668600c49697d9a2057f098)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Backpropagation\n",
    "\n",
    "* Each layer is a function of several parameters (weights): $h = f(x, \\theta)$\n",
    "* The layers could be chained. Therefore, the neural network $F$ is a composition of functions:\n",
    "$$\n",
    "F = f_k \\circ f_{k-1} \\circ \\dots \\ f_1\\\\\n",
    "h_1 = f_1(x, \\theta_1)\\\\\n",
    "h_2 = f_2(h_1, \\theta_2)\\\\\n",
    "\\dots \\\\\n",
    "\\dot{t} = f_k(h_{k-1}, \\theta_k)\n",
    "$$\n",
    "* The neural network is trained by minimizing the loss function $\\mathcal{L}$. During class we have discussed the squared-loss for linear regression, where we used $\\mathcal{L}_{\\textrm{reg}} = \\tfrac{1}{2}\\sum_n (t_n - \\dot{t}_n)^2$, where $t_n$ is the target-value of training example $n$ and $\\dot{t}_n$ the predicted value by the network/regressor.\n",
    "* Currently, the most effective way of training is to use the variation of the [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) called [Stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (and its improvements).\n",
    "* The parameters of the $m$-th layer are updated according to the following scheme:\n",
    "$$\n",
    "\\theta_m \\leftarrow \\theta_m - \\gamma \\frac{\\partial \\mathcal{L}}{\\partial \\theta_m}\n",
    "$$\n",
    "* Hyperparameter $\\gamma$ is called *learning rate*\n",
    "* As the layers are chained, the computation of $\\partial \\mathcal{L}/\\partial \\theta_m$ in advance is a complicated task. However, it is easily computed, when the forward pass is finished using the chain rule.\n",
    "* The above-stated gradient is calculated using the [chain rule](https://en.wikipedia.org/wiki/Chain_rule):\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta_m} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h_m}\n",
    "\\frac{\\partial h_m}{\\partial \\theta_m} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h_{m+1}}\n",
    "\\frac{\\partial h_{m+1}}{\\partial h_m}\n",
    "\\frac{\\partial h_m}{\\partial \\theta_m} = \\dots\n",
    "$$\n",
    "* Therefore, for each layer we have to be able to calculate several expressions:\n",
    "    * $h_m = f_m(h_{m-1}, \\theta_m)$ - the forward inference\n",
    "    * $\\partial h_{m} / \\partial h_{m-1}$ - the partial derivative of the output with respect to the input\n",
    "    * $\\partial h_{m} / \\partial \\theta_m$ - the partial derivative of the output with respect to the parameters\n",
    "* The algorithm of training of a NN using the chain rule is called [Backpropagation](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dense layer\n",
    "Dense Layer (Fully-Connected, Multiplicative) is the basic layer of a neural network. It transforms input matrix of size `(n_objects, n_in)` to the matrix of size `(n_objects, n_out)` by performing the following operation:\n",
    "$$\n",
    "H = XW + b\n",
    "$$\n",
    "or in other words:\n",
    "$$\n",
    "H_{ij} = \\sum\\limits_{k=1}^{n_\\text{in}} X_{ik}W_{kj} + b_j\n",
    "$$\n",
    "\n",
    "**Example**: \n",
    "\n",
    "You have a model of just 1 layer. The input is a point in a 3D space. And you want to predict its label: $-1$ or $1$.\n",
    "You have $75$ objects in you training subset (or batch). \n",
    "\n",
    "Therefore, $X$ has shape $75 \\times 3$. $Y$ has shape $75 \\times 1$. Weight $W$ of the layer has shape $3 \\times 1$. And $b$ is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the forward path: \n",
    "$$\n",
    "H = XW + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_forward(x_input, W, b):\n",
    "    \"\"\"Perform the mapping of the input\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the output of a dense layer \n",
    "        np.array of size `(n_objects, n_out)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check your first function. We set the matrices $X, W, b$:\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & -1 \\\\\n",
    "-1 & 0 \\\\\n",
    "\\end{bmatrix} \\quad\n",
    "W = \\begin{bmatrix}\n",
    "4 & 0\\\\\n",
    "2 & -1\\\\\n",
    "\\end{bmatrix} \\quad\n",
    "b = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And then compute \n",
    "$$\n",
    "XW = \\begin{bmatrix}\n",
    "1 & -1 \\\\\n",
    "-1 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "4 & 0 \\\\\n",
    "2 & -1\\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "2 & 1 \\\\\n",
    "-4 & 0 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "XW + b = \n",
    "\\begin{bmatrix}\n",
    "3 & 3 \\\\\n",
    "-3 & 2 \\\\\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = np.array([[1, -1],\n",
    "                   [-1, 0]])\n",
    "\n",
    "W_test = np.array([[4, 0],\n",
    "                   [2, -1]])\n",
    "\n",
    "b_test = np.array([1, 2])\n",
    "h_test =  dense_forward(X_test, W_test, b_test)\n",
    "print(h_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the chain rule: \n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial X} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial Y}\n",
    "\\frac{\\partial Y}{\\partial X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_grad_input(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of \n",
    "        the loss with respect to the input of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with \n",
    "            respect to the ouput of the dense layer \n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss with \n",
    "        respect to the input of the layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gradient of the weights:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial H}\n",
    "\\frac{\\partial H}{\\partial W} \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial H}\n",
    "\\frac{\\partial H}{\\partial b} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_grad_W(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of \n",
    "        the loss with respect to W parameter of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with \n",
    "            respect to the ouput of the dense layer \n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss \n",
    "        with respect to W parameter of the layer\n",
    "        np.array of size `(n_in, n_out)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_W\n",
    "\n",
    "def dense_grad_b(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of \n",
    "        the loss with respect to b parameter of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with \n",
    "            respect to the ouput of the dense layer \n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss \n",
    "        with respect to b parameter of the layer\n",
    "        np.array of size `(n_out,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST THE FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer Class\n",
    "\n",
    "First of all we define the basic class `Layer`. And then inherit it.\n",
    "\n",
    "We implement it for you. But `Dense` class is based on the above-written functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_phase = True\n",
    "        self.output = 0.0\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        self.output = x_input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        return grad_output\n",
    "    \n",
    "    def get_params(self):\n",
    "        return []\n",
    "    \n",
    "    def get_params_gradients(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(Dense, self).__init__()\n",
    "        #Randomly initializing the weights from normal distribution\n",
    "        self.W = np.random.normal(size=(n_input, n_output))\n",
    "        self.grad_W = np.zeros_like(self.W)\n",
    "        #initializing the bias with zero\n",
    "        self.b = np.zeros(n_output)\n",
    "        self.grad_b = np.zeros_like(self.b)\n",
    "      \n",
    "    def forward(self, x_input):\n",
    "        self.output = dense_forward(x_input, self.W, self.b)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        # get gradients of weights\n",
    "        self.grad_W = dense_grad_W(x_input, grad_output, self.W, self.b)\n",
    "        self.grad_b = dense_grad_b(x_input, grad_output, self.W, self.b)\n",
    "        # propagate the gradient backwards\n",
    "        return dense_grad_input(x_input, grad_output, self.W, self.b)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def get_params_gradients(self):\n",
    "        return [self.grad_W, self.grad_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dense_layer = Dense(2, 1)\n",
    "x_input = np.random.random((3, 2))\n",
    "y_output = dense_layer.forward(x_input)\n",
    "print(x_input)\n",
    "print(y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ReLU nonlinearity\n",
    "The dense layer is a linear layer. Combinging several linear (dense) layers is always equivalent to one dense layer see the proof below:\n",
    "$$\n",
    "H_1 = XW_1 + b_1\\\\\n",
    "H_2 = H_1W_2 + b_2\\\\\n",
    "H_2 = (XW_1 + b_1)W_2 + b_2 = X(W_1W_2) + (b_1W_2 + b_2) = XW^* + b^*\n",
    "$$\n",
    "Using multiple layers (ie a deep model) using only linear layers is equivalent to a single dense layer. A deep model using only linear layers is therefore ineffective. \n",
    "\n",
    "In order to overcome this we need to add some non-linearities. Usually they are element-wise (ie per dimension).\n",
    "$$\n",
    "H_1 = XW_1 + b_1\\\\\n",
    "H_2 = f(H_1)\\\\\n",
    "H_3 = H_2W_3 + b_3 = f(XW_1 + b_1)W_2 + b_2\\neq XW^* + b^*\n",
    "$$\n",
    "\n",
    "Nowadays, one of the most popular nonlinearity is **ReLU**:\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "It is so popular, given that it is very simple and has an easy gradient.\n",
    "\n",
    "**Example**\n",
    "\n",
    "$$\n",
    "\\text{ReLU} \\Big(\n",
    "\\begin{bmatrix}\n",
    "1 & -0.5 \\\\\n",
    "0.3 & 0.1 \n",
    "\\end{bmatrix}\n",
    "\\Big) = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0.3 & 0.1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It is a layer without trainable parameters. Just implement two functions to make it work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_forward(x_input):\n",
    "    \"\"\"relu nonlinearity\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "    # Output\n",
    "        the output of relu layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output\n",
    "\n",
    "def relu_grad_input(x_input, grad_output):\n",
    "    \"\"\"relu nonlinearity gradient. \n",
    "        Calculate the partial derivative of the loss \n",
    "        with respect to the input of the layer\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "            grad_output: np.array of size `(n_objects, n_in)`\n",
    "    # Output\n",
    "        the partial derivative of the loss \n",
    "        with respect to the input of the layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST THE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        self.output = relu_forward(x_input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        return relu_grad_input(x_input, grad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sequential model\n",
    "In order to make the work with layers more comfortable, we create `SequentialNN` - a class, which stores all its layers and performs the basic manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SequentialNN(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.training_phase = True\n",
    "        \n",
    "    def set_training_phase(is_training=True):\n",
    "        self.training_phase = is_training\n",
    "        for layer in self.layers:\n",
    "            layer.training_phase = is_training\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        self.output = x_input\n",
    "        for layer in self.layers:\n",
    "            self.output = layer.forward(self.output)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        inputs = [x_input] + [l.output for l in self.layers[:-1]]\n",
    "        for input_, layer_ in zip(inputs[::-1], self.layers[::-1]):\n",
    "            grad_output = layer_.backward(input_, grad_output)\n",
    "            \n",
    "    def get_params(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.get_params())\n",
    "        return params\n",
    "    \n",
    "    def get_params_gradients(self):\n",
    "        grads = []\n",
    "        for layer in self.layers:\n",
    "            grads.extend(layer.get_params_gradients())\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the simple neural netowrk. It takes an input of shape `(Any, 10)`. Pass it through `Dense(10, 4)`, `ReLU` and `Dense(4, 1)`. The output is a batch of size `(Any, 1)`\n",
    "```\n",
    "  INPUT\n",
    "    |\n",
    "##########\n",
    "    |\n",
    "  [ReLU]\n",
    "    |\n",
    "   ####\n",
    "    |\n",
    "    #\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = SequentialNN()\n",
    "nn.add(Dense(10, 4))\n",
    "nn.add(ReLU())\n",
    "nn.add(Dense(4, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define the loss functions. Each loss should be able to compute its value and compute its gradient with respect to the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a basic class. \n",
    "# All other losses will inherit it\n",
    "class Loss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.output = 0.0\n",
    "        \n",
    "    def forward(self, target_pred, target_true):\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, target_pred, target_true):\n",
    "        return np.zeros_like(target_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we will define [Hinge](https://en.wikipedia.org/wiki/Hinge_loss) loss function. \n",
    "$$ \n",
    "\\mathcal{L}(T, \\dot{T}) = \\frac{1}{N}\\sum\\limits_{k=1}^{N}\\max(0, 1 - \\dot{t}_k \\cdot t_k)\n",
    "$$\n",
    "\n",
    "* $N$ - number of objects\n",
    "* $\\dot{T}$ and $T$ are the vectors of length $N$. \n",
    "* $\\dot{t}_k$ is the predicted class of the $k$-th object. $\\dot{t}_k \\in {\\rm I\\!R}$\n",
    "* $t_k$ is the real class of this object. $t_k \\in \\{-1, 1\\}$\n",
    "* This loss function is used to train SVM estimators.\n",
    "\n",
    "Let's implement the calculation of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_forward(target_pred, target_true):\n",
    "    \"\"\"Compute the value of Hinge loss \n",
    "        for a given prediction and the ground truth\n",
    "    # Arguments\n",
    "        target_pred: predictions - np.array of size `(n_objects,)`\n",
    "        target_true: ground truth - np.array of size `(n_objects,)`\n",
    "    # Output\n",
    "        the value of Hinge loss \n",
    "        for a given prediction and the ground truth\n",
    "        scalar\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should compute the gradient of the loss function with respect to its input. It is a vector with the same shape as the input.\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{T}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{t}_1} \\\\ \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{t}_2} \\\\ \n",
    "\\vdots \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{t}_N} \\\\ \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_grad_input(target_pred, target_true):\n",
    "    \"\"\"Compute the partial derivative \n",
    "        of Hinge loss with respect to its input\n",
    "    # Arguments\n",
    "        target_pred: predictions - np.array of size `(n_objects,)`\n",
    "        target_true: ground truth - np.array of size `(n_objects,)`\n",
    "    # Output\n",
    "        the partial derivative \n",
    "        of Hinge loss with respect to its input\n",
    "        np.array of size `(n_objects,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Hinge(Loss):\n",
    "    \n",
    "    def forward(self, target_pred, target_true):\n",
    "        self.output = hinge_forward(target_pred, target_true)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, target_pred, target_true):\n",
    "        return hinge_grad_input(target_pred, target_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TEST FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. $L_2$ Regularization & Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways of the regularization of a model. They are used to avoid learning models which behave well on the training subset and fail during testing. We will implement [$L_2$ regularization](http://www.deeplearningbook.org/contents/regularization.html) also known as weight decay.\n",
    "\n",
    "The key idea of $L_2$ regularization is to add an extra term to the loss functions:\n",
    "$$\n",
    "\\mathcal{L}^* = \\mathcal{L} + \\frac{\\lambda}{2} \\|\\theta\\|^2_2\n",
    "$$\n",
    "\n",
    "For some cases only the weights of a single layer are penalized, but we will penalize all the weights.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^* = \\mathcal{L} + \\frac{\\lambda}{2} \\sum\\limits_{m=1}^k \\|\\theta_m\\|^2_2\n",
    "$$\n",
    "\n",
    "Therefore, the updating scheme is also modified\n",
    "\n",
    "$$\n",
    "\\theta_m \\leftarrow \\theta_m - \\gamma \\frac{\\partial \\mathcal{L}^*}{\\partial \\theta_m}\\\\\n",
    "\\frac{\\partial \\mathcal{L}^*}{\\partial \\theta_m} = \\frac{\\partial \\mathcal{L}}{\\partial \\theta_m} + \\lambda \\theta_m\\\\\n",
    "\\theta_m \\leftarrow \\theta_m - \\gamma \\frac{\\partial \\mathcal{L}}{\\partial \\theta_m} - \\lambda \\theta_m\n",
    "$$\n",
    "\n",
    "As you can see, the updating scheme also gets an extra term. $\\lambda$ is the coefficient of the weight decay. \n",
    "\n",
    "The update of the weights would be implemented later in `Optimizer` class. Here you should implement the computation of $L_2$ norm of the weights from the given list.\n",
    "$$\n",
    "f(\\lambda, [\\theta_1, \\theta_2, \\dots, \\theta_k]) = \\frac{\\lambda}{2} \\sum\\limits_{m=1}^k \\|\\theta_m\\|^2_2\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l2_regularizer(weight_decay, weights):\n",
    "    \"\"\"Compute the L2 regularization term\n",
    "    # Arguments\n",
    "        weight_decay: float\n",
    "        weights: list of arrays of different shapes\n",
    "    # Output\n",
    "        sum of the L2 norms of the input weights\n",
    "        scalar\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Optimizer\n",
    "\n",
    "We implement the optimizer to perform the updates of the weights according to the certain scheme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    '''This is a basic class. \n",
    "    All other optimizers will inherit it\n",
    "    '''\n",
    "    def __init__(self, model, lr=0.01, weight_decay=0.0):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "    def update_params(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    '''Stochastic gradient descent optimizer\n",
    "    https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "    '''\n",
    "        \n",
    "    def update_params(self):\n",
    "        weights = self.model.get_params()\n",
    "        grads = self.model.get_params_gradients()\n",
    "        for w, dw in zip(weights, grads):\n",
    "            update = self.lr * dw + self.weight_decay * w\n",
    "            # it writes the result to the previous variable instead of copying\n",
    "            np.subtract(w, update, out=w) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Advanced blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an optional section. If you liked the process of understanding NNs by implementing them from scratch, here are several more tasks for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Dropout\n",
    "\n",
    "[Dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) is a method of regularization. It is also could be interpreted as the augmentation method. The key idea is to randomly drop some values of the input tensor. It avoids overfitting of the model. Its behaviour is different during the training and testing.\n",
    "\n",
    "![dtopout](./src/dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, you should implement the method of calculating the binary mask. The binary mask has the same shape as the input. The mask could have the value 0 for the certain element with the probability `drop_rate` and 1 - with  `1.0 - drop_rate`. **None**, in $p$ from article is `1.0 - drop_rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_generate_mask(shape, drop_rate):\n",
    "    \"\"\"Generate mask \n",
    "    # Arguments\n",
    "        shape: shape of the input array \n",
    "            tuple \n",
    "        drop_rate: probability of the element \n",
    "            to be multiplied by 0\n",
    "            scalar\n",
    "    # Output\n",
    "        binary mask \n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the above-described operation of mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_forward(x_input, mask, drop_rate, training_phase):\n",
    "    \"\"\"Perform the mapping of the input\n",
    "    # Arguments\n",
    "        x_input: input of the layer \n",
    "            np.array of size `(n_objects, n_in)`\n",
    "        mask: binary mask\n",
    "            np.array of size `(n_objects, n_in)`\n",
    "        drop_rate: probability of the element to be multiplied by 0\n",
    "            scalar\n",
    "        training_phase: bool eiser `True` - training, or `False` - testing\n",
    "    # Output\n",
    "        the output of the dropout layer \n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as usual, implement the calculation of the partial derivative of the loss function with respect to the input of a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_grad_input(x_input, grad_output, mask):\n",
    "    \"\"\"Calculate the partial derivative of \n",
    "        the loss with respect to the input of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with \n",
    "            respect to the ouput of the dropout layer \n",
    "            np.array of size `(n_objects, n_in)`\n",
    "        mask: binary mask\n",
    "            np.array of size `(n_objects, n_in)`\n",
    "    # Output\n",
    "        the partial derivative of the loss with \n",
    "        respect to the input of the layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    \n",
    "    def __init__(self, drop_rate):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.mask = 1.0\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        if self.training_phase:\n",
    "            self.mask = dropout_generate_mask(x_input.shape, self.drop_rate)\n",
    "        self.output = dropout_forward(x_input, self.mask, \n",
    "                                      self.drop_rate, self.training_phase)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        grad_input = dropout_grad_input(x_input, grad_output, self.mask)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8.2 MSE Loss\n",
    "MSE (Mean Squared Error) is a popular loss for the regression tasks.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(T, \\dot{T}) = \\frac{1}{2N}\\sum\\limits_{k=1}^N(t_k - \\dot{t}_k)^2\n",
    "$$\n",
    "\n",
    "* $N$ - number of objects\n",
    "* $\\dot{T}$ and $T$ are the vectors of length $N$. \n",
    "* $\\dot{t}_k$ is the predicted target value of the $k$-th object. $\\dot{t}_k \\in {\\rm I\\!R}$\n",
    "* $t_k$ is the real target value of the $k$-th object. $t_k \\in {\\rm I\\!R}$\n",
    "* This loss function is used to train regressors.\n",
    "\n",
    "Let's implement the calculation of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse_forward(target_pred, target_true):\n",
    "    \"\"\"Compute the value of MSE loss\n",
    "        for a given prediction and the ground truth\n",
    "    # Arguments\n",
    "        target_pred: predictions - np.array of size `(n_objects,)`\n",
    "        target_true: ground truth - np.array of size `(n_objects,)`\n",
    "    # Output\n",
    "        the value of MSE loss \n",
    "        for a given prediction and the ground truth\n",
    "        scalar\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should compute the gradient of the loss function with respect to its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse_grad_input(target_pred, target_true):\n",
    "    \"\"\"Compute the partial derivative \n",
    "        of MSE loss with respect to its input\n",
    "    # Arguments\n",
    "        target_pred: predictions - np.array of size `(n_objects,)`\n",
    "        target_true: ground truth - np.array of size `(n_objects,)`\n",
    "    # Output\n",
    "        the partial derivative \n",
    "        of MSE loss with respect to its input\n",
    "        np.array of size `(n_objects,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSE(Loss):\n",
    "    \n",
    "    def forward(self, target_pred, target_true):\n",
    "        self.output = mse_forward(target_pred, target_true)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, target_pred, target_true):\n",
    "        return mse_grad_input(target_pred, target_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
