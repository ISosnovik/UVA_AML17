{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "## Blocks\n",
    "\n",
    "The main idea of this assignment is to allow you to undestand how **neural networks** (NNs) work. We will cover the main aspects such as the **Backpropagation** and **Optimization Methods**. All mathematical operations should be implemented in **NumPy** only. \n",
    "\n",
    "The assignmnent consist of 2 notebooks:\n",
    "* *Blocks* - the place where the main **building blocks** of the NNs should be implemented\n",
    "* *Experiments* - a playground. There we will train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* [0. Preliminaries](#0.-Preliminaries)\n",
    "* [1. Backpropagation](#1.-Backpropagation)\n",
    "* [2. Dense layer](#2.-Dense-layer)\n",
    "* [3. ReLU nonlinearity](#3.-ReLU-nonlinearity)\n",
    "* [4. Sequential model](#4.-Sequential-model)\n",
    "* [5. Loss](#5.-Loss)\n",
    "* [6. $L_2$ Regularization & Weight Decay](#6.-$L_2$-Regularization-&-Weight-Decay)\n",
    "* [7. Optimizer](#7.-Optimizer)\n",
    "* [8. ★ What else?](#8.-★-What-else?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preliminaries\n",
    "In this assignment we will use **classes** and their **instances** (objects). It will allow us to write less code and make it more readable. However, you don't have not take care about the exact implementation of the classes. We did it for you. \n",
    "But if you are interested in it, here are some useful links:\n",
    "* The official [documentation](https://docs.python.org/3/tutorial/classes.html) \n",
    "* Video by *sentdex*: [Object Oriented Programming Introduction](https://www.youtube.com/watch?v=ekA6hvk-8H8)\n",
    "* Antipatterns in OOP: [Stop Writing Classes](https://www.youtube.com/watch?v=o9pEzgHorH0)\n",
    "\n",
    "The interface of the current blocks is mostly inspired by **[Torch](http://torch.ch) / [PyTorch](http://pytorch.org)**. You can also take a look at the first implementation of [Keras](https://github.com/fchollet/keras/tree/37a1db225420851cc668600c49697d9a2057f098)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Backpropagation\n",
    "\n",
    "* Each layer is a function of several parameters (weights): $f = f(x, \\theta)$\n",
    "* The layers could be chained. Therefore, the neural network $F$ is a composition of functions:\n",
    "$$\n",
    "F = f_k \\circ f_{k-1} \\circ \\dots \\ f_1\\\\\n",
    "y_1 = f_1(x, \\theta_1)\\\\\n",
    "y_2 = f_2(y_1, \\theta_2)\\\\\n",
    "\\dots \\\\\n",
    "y_k = f_k(y_{k-1}, \\theta_k)\n",
    "$$\n",
    "* The neural network is trained by minimizing the loss function $\\mathcal{L}$. \n",
    "* Currently, the most effective way of training is to use the variation of the [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) called [Stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) and its improvements.\n",
    "* The parameters of the $m$-th layer are updated according to the following scheme:\n",
    "$$\n",
    "\\theta_m \\leftarrow \\theta_m - \\gamma \\frac{\\partial \\mathcal{L}}{\\partial \\theta_m}\n",
    "$$\n",
    "* Hyperparameter $\\gamma$ is called *learning rate*\n",
    "* As the layers are chained, the computation of $\\partial \\mathcal{L}/\\partial \\theta_m$ in advance is a complicated task. \n",
    "* The above-stated gradient is calculated using the [chain rule](https://en.wikipedia.org/wiki/Chain_rule):\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta_m} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y_m}\n",
    "\\frac{\\partial y_m}{\\partial \\theta_m} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y_{m+1}}\n",
    "\\frac{\\partial y_{m+1}}{\\partial y_m}\n",
    "\\frac{\\partial y_m}{\\partial \\theta_m} = \\dots\n",
    "$$\n",
    "* Therefore, each layer have to be able to calculate several expressions:\n",
    "    * $y_m = f_m(y_{m-1}, \\theta_m)$ - mapping of the input\n",
    "    * $\\partial y_{m} / \\partial y_{m-1}$ - the partial derivative of the output with respect to the input\n",
    "    * $\\partial y_{m} / \\partial \\theta_m$ - the partial derivative of the output with respect its parameters\n",
    "* The algorithm of training of a NN using the chain rule is called [Backpropagation](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dense layer\n",
    "Dense Layer (Fully-Connected, Multiplicative) is the basic layer of a neural network. It transforms input matrix of size `(n_objects, n_in)` to the matrix of size `(n_objects, n_out)` by performing the following operation:\n",
    "$$\n",
    "Y = XW + b\n",
    "$$\n",
    "or in other words:\n",
    "$$\n",
    "Y_{ij} = \\sum\\limits_{k=1}^{n_\\text{in}} X_{ik}W_{kj} + b_j\n",
    "$$\n",
    "\n",
    "**Example**: \n",
    "\n",
    "You have a model of just 1 layer. The input is a point in a 3D space. And you want to predict its label: $-1$ or $1$.\n",
    "You have $75$ objects in you training subset (or batch). \n",
    "\n",
    "Therefore, $X$ has shape $75 \\times 3$. $Y$ has shape $75 \\times 1$. Weight $W$ of the layer has shape $3 \\times 1$. And $b$ is a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the forward path: \n",
    "$$\n",
    "Y = XW + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_forward(x_input, W, b):\n",
    "    \"\"\"Matrix multiplication operation\n",
    "    # Arguements\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        np.array of size `(n_objects, n_out)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the chain rule: \n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial X} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial Y}\n",
    "\\frac{\\partial Y}{\\partial X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_grad_input(x_input, grad_output, W, b):\n",
    "    \"\"\"Matrix multiplication gradient\n",
    "    # Arguements\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "        grad_output: np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gradient of the weights:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial Y}\n",
    "\\frac{\\partial Y}{\\partial W} \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial Y}\n",
    "\\frac{\\partial Y}{\\partial b} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_grad_W(x_input, grad_output, W, b):\n",
    "    \"\"\"W gradient computation\n",
    "    # Arguements\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "        grad_output: np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        np.array of size `(n_in, n_out)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_W\n",
    "\n",
    "def dense_grad_b(x_input, grad_output, W, b):\n",
    "    \"\"\"b gradient computation\n",
    "    # Arguements\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "        grad_output: np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        np.array of size `(n_out,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST THE FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we define the basic class `Layer`. And then inherit it.\n",
    "\n",
    "We implement it for you. But `Dense` class is based on the above-written functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_phase = True\n",
    "        self.output = 0.0\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        self.output = x_input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        return grad_output\n",
    "    \n",
    "    def get_params(self):\n",
    "        return []\n",
    "    \n",
    "    def get_params_gradients(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(Dense, self).__init__()\n",
    "        #Randomly initializing the weights from normal distribution\n",
    "        self.W = np.random.normal(size=(n_input, n_output))\n",
    "        self.grad_W = np.zeros_like(self.W)\n",
    "        #initializing the bias with zero\n",
    "        self.b = np.zeros(n_output)\n",
    "        self.grad_b = np.zeros_like(self.b)\n",
    "      \n",
    "    def forward(self, x_input):\n",
    "        self.output = dense_forward(x_input, self.W, self.b)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        # get gradients of weights\n",
    "        self.grad_W = dense_grad_W(x_input, grad_output, self.W, self.b)\n",
    "        self.grad_b = dense_grad_b(x_input, grad_output, self.W, self.b)\n",
    "        # propagate the gradient backwards\n",
    "        return dense_grad_input(x_input, grad_output, self.W, self.b)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def get_params_gradients(self):\n",
    "        return [self.grad_W, self.grad_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dense_layer = Dense(2, 1)\n",
    "x_input = np.random.random((3, 2))\n",
    "y_output = dense_layer.forward(x_input)\n",
    "print(x_input)\n",
    "print(y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ReLU nonlinearity\n",
    "The combination of several dense layers is equivalent to one dense layer:\n",
    "$$\n",
    "Y_1 = XW_1 + b_1\\\\\n",
    "Y_2 = Y_1W_2 + b_2\\\\\n",
    "Y_2 = (XW_1 + b_1)W_2 + b_2 = X(W_1W_2) + (b_1W_2 + b_2) = XW^* + b^*\n",
    "$$\n",
    "It means that the training of such model is ineffective. \n",
    "\n",
    "In order to overcome this collapse, nonlinear functions are used. Usually they are element-wise.\n",
    "$$\n",
    "Y_1 = XW_1 + b_1\\\\\n",
    "Y_2 = f(Y_1)\\\\\n",
    "Y_3 = Y_2W_3 + b_3 = f(XW_1 + b_1)W_2 + b_2\\neq XW^* + b^*\n",
    "$$\n",
    "\n",
    "The most popular nonlinearity is **ReLU**:\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "**Example**\n",
    "\n",
    "$$\n",
    "\\text{ReLU} \\Big(\n",
    "\\begin{bmatrix}\n",
    "1 & -0.5 \\\\\n",
    "0.3 & 0.1 \n",
    "\\end{bmatrix}\n",
    "\\Big) = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0.3 & 0.1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It is a layer without trainable parameters. Just implement two functions to make it work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_forward(x_input):\n",
    "    \"\"\"relu nonlinearity\n",
    "    # Arguements\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output\n",
    "\n",
    "def relu_grad_input(x_input, grad_output):\n",
    "    \"\"\"relu nonlinearity gradient\n",
    "    # Arguements\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "            grad_output: np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST THE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        self.output = relu_forward(x_input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        return relu_grad_input(x_input, grad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sequential model\n",
    "In order to make the work with layers more comfortable, we create `SequentialNN` - a class, which stores all its layers and performs the basic manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SequentialNN(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        self.output = x_input\n",
    "        for layer in self.layers:\n",
    "            self.output = layer.forward(self.output)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        inputs = [x_input] + [l.output for l in self.layers[:-1]]\n",
    "        for input_, layer_ in zip(inputs[::-1], self.layers[::-1]):\n",
    "            grad_output = layer_.backward(input_, grad_output)\n",
    "            \n",
    "    def get_params(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.get_params())\n",
    "        return params\n",
    "    \n",
    "    def get_params_gradients(self):\n",
    "        grads = []\n",
    "        for layer in self.layers:\n",
    "            grads.extend(layer.get_params_gradients())\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the simple neural netowrk. It takes an input of shape `(Any, 10)`. Pass it through `Dense(10, 4)`, `ReLU` and `Dense(4, 1)`. The output is a batch of size `(Any, 1)`\n",
    "```\n",
    "  INPUT\n",
    "    |\n",
    "##########\n",
    "    |\n",
    "  [ReLU]\n",
    "    |\n",
    "   ####\n",
    "    |\n",
    "    #\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = SequentialNN()\n",
    "nn.add(Dense(10, 4))\n",
    "nn.add(ReLU())\n",
    "nn.add(Dense(4, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define the loss functions. Each loss should be able to compute its value and compute its gradient with respect to the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a basic class. \n",
    "# All other losses will inherit it\n",
    "class Loss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.output = 0.0\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        return np.zeros_like(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we will define [Hinge](https://en.wikipedia.org/wiki/Hinge_loss) loss function. \n",
    "$$ \n",
    "\\mathcal{L}(y^{\\text{pred}}, y^{\\text{true}}) = \\frac{1}{N}\\sum\\limits_{i=1}^{N}\\max(0, 1 - y_i^{\\text{pred}} \\cdot y_i^{\\text{true}})\n",
    "$$\n",
    "\n",
    "* $N$ - number of objects\n",
    "* $y^{\\text{pred}}$ and $y^{\\text{true}}$ are the vectors of length $N$. \n",
    "* $y_i^{\\text{pred}}$ is a predicted class of the $i$-th object. $y_i^{\\text{pred}} \\in {\\rm I\\!R}$\n",
    "* $y_i^{\\text{true}}$ is a real class of this object. $y_i^{\\text{true}} \\in \\{-1, 1\\}$\n",
    "* This loss function is used to train SVM estimators.\n",
    "\n",
    "Let's implement the calculation of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_forward(y_pred, y_true):\n",
    "    \"\"\"Computation of Hinge loss\n",
    "    # Arguements\n",
    "        y_pred: np.array of size `(n_objects,)`\n",
    "        y_true: np.array of size `(n_objects,)`\n",
    "    # Output\n",
    "        scalar\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should compute the gradient of the loss function with respect to its input. It is a vector with the same shape as the input.\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y^{\\text{pred}}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y_1^{\\text{pred}}} \\\\ \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y_2^{\\text{pred}}} \\\\ \n",
    "\\vdots \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y_N^{\\text{pred}}} \\\\ \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_grad_input(y_pred, y_true):\n",
    "    \"\"\"The gradient of Hinge loss with respect to its input\n",
    "    # Arguements\n",
    "        y_pred: np.array of size `(n_objects,)`\n",
    "        y_true: np.array of size `(n_objects,)`\n",
    "    # Output\n",
    "        np.array of size `(n_objects,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Hinge(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.output = hinge_forward(y_pred, y_true)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        return hinge_grad_input(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TEST FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. $L_2$ Regularization & Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways of the regularization of a model. They are used to avoid learning models which behave well on the training subset and fail during testing. We will implement [$L_2$ regularization](http://www.deeplearningbook.org/contents/regularization.html) also known as weight decay.\n",
    "\n",
    "The key idea of $L_2$ regularization is to add an extra term to the loss functions:\n",
    "$$\n",
    "\\mathcal{L}^* = \\mathcal{L} + \\frac{\\lambda}{2} \\|\\theta\\|^2_2\n",
    "$$\n",
    "\n",
    "For some cases only the weights of a single layer are penalized, but we will penalize all the weights.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^* = \\mathcal{L} + \\frac{\\lambda}{2} \\sum\\limits_{m=1}^k \\|\\theta_m\\|^2_2\n",
    "$$\n",
    "\n",
    "Therefore, the updating scheme is also modified\n",
    "\n",
    "$$\n",
    "\\theta_m \\leftarrow \\theta_m - \\gamma \\frac{\\partial \\mathcal{L}^*}{\\partial \\theta_m}\\\\\n",
    "\\frac{\\partial \\mathcal{L}^*}{\\partial \\theta_m} = \\frac{\\partial \\mathcal{L}}{\\partial \\theta_m} + \\lambda \\theta_m\\\\\n",
    "\\theta_m \\leftarrow \\theta_m - \\gamma \\frac{\\partial \\mathcal{L}}{\\partial \\theta_m} - \\lambda \\theta_m\n",
    "$$\n",
    "\n",
    "As you can see, the updating scheme also gets an extra term. $\\lambda$ is the coefficient of the weight decay. \n",
    "\n",
    "The update of the weights would be implemented later in `Optimizer` class. Here you should implement the computation of $L_2$ norm of the weights from the given list.\n",
    "$$\n",
    "f(\\lambda, [\\theta_1, \\theta_2, \\dots, \\theta_k]) = \\frac{\\lambda}{2} \\sum\\limits_{m=1}^k \\|\\theta_m\\|^2_2\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l2_regularizer(weight_decay, weights):\n",
    "    \"\"\"Computation of the L2 regularization term\n",
    "    # Arguements\n",
    "        weight_decay: float\n",
    "        weights: list of arrays of different shapes\n",
    "    # Output\n",
    "        scalar\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Optimizer\n",
    "\n",
    "We implement the optimizer to perform the updates of the weights according to the certain scheme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    '''This is a basic class. \n",
    "    All other optimizer will inherit it\n",
    "    '''\n",
    "    def __init__(self, model, lr=0.01, weight_decay=0.0):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "    def update_params(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    '''Stochastic gradient descent optimizer\n",
    "    https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "    '''\n",
    "        \n",
    "    def update_params(self):\n",
    "        weights = self.model.get_params()\n",
    "        grads = self.model.get_params_gradients()\n",
    "        for w, dw in zip(weights, grads):\n",
    "            update = self.lr * dw + self.weight_decay * w\n",
    "            # it writes the result to the previous variable instead of copying\n",
    "            np.subtract(w, update, out=w) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. What else?  (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is an optional section. If you liked the process of understanding NNs by implementing them from scratch, here are several more tasks for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    \n",
    "    def __init__(self, drop_rate):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.mask = 1.0\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        if self.training_phase:\n",
    "            self.mask = ## GENERATE THE MASK ##\n",
    "            self.output = ## COMPUTE THE OUTPUT DURING TRAINING ##\n",
    "        else:\n",
    "            self.output = ## COMPUTE THE OUTPUT DURING TESTING ##\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        grad_input = ## COMPUTE THE GRADIENT OF THE OUTPUT WITH RESPECT TO THE INPUT ##\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
